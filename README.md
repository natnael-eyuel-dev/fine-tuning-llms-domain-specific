# Fine-Tuning Large Language Models for Domain-Specific Tasks

Runnables and artifacts for fine-tuning and evaluating a Transformer on a domain dataset.

## What this project does

We fine-tune a small Transformer encoder (DistilBERT) on a **biomedical domain** dataset (`pubmed_rct`) for **section classification**.

- **Baseline**: TFâ€‘IDF + Logistic Regression
- **Fine-tuned model**: `distilbert-base-uncased` (Hugging Face `Trainer`)
- **Metrics**: Accuracy + Macro F1

## Run in Colab (recommended)

1. Clone the repo in Colab (or upload the whole folder).
2. Open and run:
   - `notebooks/finetune_pubmed_rct.ipynb`

The notebook installs dependencies from `requirements.txt` (Colab), trains baseline + fine-tuned model, and generates all artifacts automatically.

## Run locally

### 1) Create env + install deps

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

### 2) Run the notebook

- Open: `notebooks/finetune_pubmed_rct.ipynb`
- Run all cells

## Outputs (generated)

Main:

- `artifacts/metrics.json`
- `artifacts/examples.json`

Extra (generated by scripts; the notebook runs them at the end):

- `artifacts/data_checks.json`
- `artifacts/training_curves.png`
- `artifacts/confusion_matrix.png`
- `artifacts/classification_report.json`
- `artifacts/failures.json`
- `reports/evaluation_report_filled.md`

## Repo structure

- `notebooks/`: runnable notebook
- `src/`: reusable code (dataset, baseline, fine-tuning)
- `scripts/`: helpers to generate curves/report/error analysis
- `reports/`: report templates + generated report
- `reference/`: task reference materials

## Notes

- If you don't have a GPU, the notebook automatically uses smaller sample sizes so it still runs on CPU.


