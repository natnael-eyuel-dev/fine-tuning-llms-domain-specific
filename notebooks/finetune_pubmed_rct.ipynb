{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning LLMs for a Domain-Specific Classification Task\n",
        "\n",
        "This notebook is the **runnable code deliverable** from the slide deck.\n",
        "\n",
        "## What we will do\n",
        "\n",
        "- Load a **biomedical domain dataset** from Hugging Face: `OxAISH-AL-LLM/pubmed_20k_rct`\n",
        "- Train a strong classical baseline: **TFâ€‘IDF + Logistic Regression**\n",
        "- Fine-tune a Transformer encoder: **PubMedBERT** (biomedical BERT)\n",
        "- Evaluate both on a held-out **test** split using **Accuracy** and **Macro F1**\n",
        "- Save outputs to:\n",
        "  - `artifacts/metrics.json`\n",
        "  - `artifacts/examples.json`\n",
        "\n",
        "## Notes\n",
        "\n",
        "- If you are on **CPU**, the notebook automatically subsamples the dataset so it finishes in reasonable time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a293a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# colab clone\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "REPO_URL = \"https://github.com/<YOUR_USERNAME>/<YOUR_REPO>.git\"\n",
        "REPO_DIR = \"<YOUR_REPO>\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not (Path.cwd() / \"src\").exists():\n",
        "        os.chdir(\"/content\")\n",
        "        if not Path(REPO_DIR).exists():\n",
        "            !git clone {REPO_URL}\n",
        "        os.chdir(REPO_DIR)\n",
        "\n",
        "print(\"cwd:\", Path.cwd())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from dataclasses import replace\n",
        "from pathlib import Path\n",
        "\n",
        "# repo path setup\n",
        "ROOT = Path.cwd().resolve()\n",
        "if ROOT.name == \"notebooks\":\n",
        "    ROOT = ROOT.parent\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "if not (ROOT / \"src\").exists():\n",
        "    raise RuntimeError(\"src/ folder not found. in colab you need to clone the repo first.\")\n",
        "\n",
        "# colab install\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "\n",
        "    req_path = ROOT / \"requirements.txt\"\n",
        "    if not req_path.exists():\n",
        "        raise RuntimeError(f\"requirements.txt not found: {req_path}\")\n",
        "\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"pip\"])\n",
        "\n",
        "    # dont reinstall torch in colab\n",
        "    filtered = []\n",
        "    for line in req_path.read_text(encoding=\"utf-8\").splitlines():\n",
        "        s = line.strip()\n",
        "        if not s or s.startswith(\"#\"):\n",
        "            continue\n",
        "        if s == \"torch\" or s.startswith(\"torch\"):\n",
        "            continue\n",
        "        filtered.append(line)\n",
        "\n",
        "    tmp_req = ROOT / \"requirements-colab.txt\"\n",
        "    tmp_req.write_text(\"\\n\".join(filtered) + \"\\n\", encoding=\"utf-8\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(tmp_req)])\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# hide optional hub auth warning (token is optional for public models/datasets)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"The secret `HF_TOKEN` does not exist*\",\n",
        "    category=UserWarning,\n",
        ")\n",
        "logging.getLogger(\"huggingface_hub.utils._http\").setLevel(logging.ERROR)\n",
        "\n",
        "import torch\n",
        "\n",
        "from src.baseline import train_tfidf_logreg_baseline\n",
        "from src.config import TrainConfig\n",
        "from src.data import dataset_preview, load_hf_classification_dataset\n",
        "from src.finetune import finetune_distilbert_classifier\n",
        "from src.reporting import write_json\n",
        "\n",
        "# save outputs here\n",
        "ARTIFACTS = ROOT / \"artifacts\"\n",
        "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cfg = TrainConfig()\n",
        "\n",
        "# cpu run (smaller)\n",
        "if not torch.cuda.is_available():\n",
        "    cfg = replace(cfg, max_train_samples=2000, max_eval_samples=1000, max_test_samples=1000)\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Config:\", cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds, text_field, id2label = load_hf_classification_dataset(\n",
        "    dataset_name=cfg.dataset_name,\n",
        "    dataset_config=cfg.dataset_config,\n",
        "    text_field=cfg.text_field,\n",
        "    label_field=cfg.label_field,\n",
        "    max_train_samples=cfg.max_train_samples,\n",
        "    max_eval_samples=cfg.max_eval_samples,\n",
        "    max_test_samples=cfg.max_test_samples,\n",
        "    seed=cfg.seed,\n",
        ")\n",
        "\n",
        "print('Resolved text field:', text_field)\n",
        "print('Labels:', id2label)\n",
        "print('Sizes:', {k: len(v) for k, v in ds.items()})\n",
        "print(json.dumps(dataset_preview(ds, text_field=text_field, label_field=cfg.label_field, k=2), indent=2)[:2000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline: TF-IDF + Logistic Regression\n",
        "baseline_result, baseline_metrics = train_tfidf_logreg_baseline(\n",
        "    ds,\n",
        "    text_field=text_field,\n",
        "    label_field=cfg.label_field,\n",
        "    max_features=cfg.baseline_max_features,\n",
        "    seed=cfg.seed,\n",
        ")\n",
        "print(\"Baseline (test):\", baseline_result)\n",
        "print(\"Baseline metrics:\", baseline_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune model\n",
        "ft_result, ft_metrics = finetune_distilbert_classifier(\n",
        "    ds,\n",
        "    text_field=text_field,\n",
        "    label_field=cfg.label_field,\n",
        "    model_name=cfg.model_name,\n",
        "    max_length=cfg.max_length,\n",
        "    output_dir=cfg.output_dir,\n",
        "    num_train_epochs=cfg.num_train_epochs,\n",
        "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=cfg.per_device_eval_batch_size,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    weight_decay=cfg.weight_decay,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    logging_steps=cfg.logging_steps,\n",
        "    eval_strategy=cfg.eval_strategy,\n",
        "    save_strategy=cfg.save_strategy,\n",
        "    seed=cfg.seed,\n",
        "    id2label=id2label,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    early_stopping_patience=cfg.early_stopping_patience,\n",
        "    early_stopping_threshold=cfg.early_stopping_threshold,\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned (test):\", ft_result)\n",
        "print(\"Fine-tuned metrics:\", ft_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save combined metrics\n",
        "metrics_path = ARTIFACTS / \"metrics.json\"\n",
        "write_json(\n",
        "    metrics_path,\n",
        "    {\n",
        "        \"dataset\": {\n",
        "            \"name\": cfg.dataset_name,\n",
        "            \"config\": cfg.dataset_config,\n",
        "            \"text_field\": text_field,\n",
        "            \"splits\": {k: len(v) for k, v in ds.items()},\n",
        "        },\n",
        "        \"baseline\": {\n",
        "            \"test_accuracy\": baseline_result.accuracy,\n",
        "            \"test_macro_f1\": baseline_result.macro_f1,\n",
        "            **baseline_metrics,\n",
        "        },\n",
        "        \"finetune\": {\n",
        "            \"model_dir\": str(ft_result.model_dir),\n",
        "            \"test_accuracy\": ft_result.test_accuracy,\n",
        "            \"test_macro_f1\": ft_result.test_macro_f1,\n",
        "            **ft_metrics,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "print(\"Wrote:\", metrics_path)\n",
        "print(metrics_path.read_text(encoding=\"utf-8\")[:1500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a few example predictions using the fine-tuned model\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_dir = Path(ft_result.model_dir)\n",
        "tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "mdl = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "mdl.eval()\n",
        "\n",
        "def predict_one(text: str) -> int:\n",
        "    enc = tok(text, return_tensors=\"pt\", truncation=True, max_length=cfg.max_length)\n",
        "    with torch.no_grad():\n",
        "        out = mdl(**enc)\n",
        "    return int(out.logits.argmax(dim=-1).item())\n",
        "\n",
        "examples = []\n",
        "for i in range(min(12, len(ds[\"test\"]))):\n",
        "    text = ds[\"test\"][i][text_field]\n",
        "    true = int(ds[\"test\"][i][cfg.label_field])\n",
        "    pred = predict_one(text)\n",
        "    examples.append(\n",
        "        {\n",
        "            \"text\": text,\n",
        "            \"true_id\": true,\n",
        "            \"pred_id\": pred,\n",
        "            \"true_label\": id2label.get(true) if id2label else None,\n",
        "            \"pred_label\": id2label.get(pred) if id2label else None,\n",
        "            \"correct\": bool(true == pred),\n",
        "        }\n",
        "    )\n",
        "\n",
        "examples_path = ARTIFACTS / \"examples.json\"\n",
        "write_json(examples_path, {\"examples\": examples})\n",
        "print(\"Wrote:\", examples_path)\n",
        "print(examples_path.read_text(encoding=\"utf-8\")[:1500])\n",
        "\n",
        "# run extra scripts (make curves + report)\n",
        "import subprocess\n",
        "\n",
        "for rel in [\n",
        "    \"scripts/dataset_checks.py\",\n",
        "    \"scripts/plot_training_curves.py\",\n",
        "    \"scripts/error_analysis.py\",\n",
        "    \"scripts/generate_evaluation_report.py\",\n",
        "]:\n",
        "    subprocess.run([sys.executable, str(ROOT / rel)], check=True)\n",
        "\n",
        "print(\"done\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
